{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade transformers datasets accelerate evaluate -q\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\nimport torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    Trainer,\n    TrainingArguments,\n)\nfrom datasets import Dataset\n\n# --------------------\n# CONFIG\n# --------------------\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\nMODELS = [\n    \"bert-base-uncased\",\n    \"roberta-base\",\n    \"microsoft/deberta-v3-base\",\n    \"google/electra-base-discriminator\",\n    \"distilbert-base-uncased\"\n]\nMAX_LENGTH = 256\nBATCH_SIZE = 16\nEPOCHS = 3\nLR = 2e-5\n\n# --------------------\n# LOAD DATA (Kaggle Path)\n# --------------------\nprint(\"Loading data...\")\ndata_path = \"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\"\ndf = pd.read_csv(data_path)\ndf['label'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n\n# --------------------\n# TRAIN/VAL/TEST SPLIT (Subset for model comparison)\n# --------------------\ndf_subset, _ = train_test_split(df, train_size=8000, stratify=df['label'], random_state=SEED)\ntrain_df, temp_df = train_test_split(df_subset, train_size=0.7, stratify=df_subset['label'], random_state=SEED)\nval_df, test_df = train_test_split(temp_df, train_size=0.5, stratify=temp_df['label'], random_state=SEED)\n\n# Convert to Dataset objects\ndef df_to_dataset(dataframe, tokenizer):\n    return Dataset.from_pandas(dataframe).map(\n        lambda x: tokenizer(x['review'], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH),\n        batched=True\n    )\n\n# --------------------\n# METRICS FUNCTION\n# --------------------\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=-1)\n    f1 = f1_score(labels, preds, average='macro')\n    acc = accuracy_score(labels, preds)\n    prec = precision_score(labels, preds, average='macro')\n    rec = recall_score(labels, preds, average='macro')\n    return {\"f1\": f1, \"accuracy\": acc, \"precision\": prec, \"recall\": rec}\n\n# --------------------\n# TRAINING LOOP FOR SUBSET EXPERIMENTS\n# --------------------\nresults = []\nfor model_name in MODELS:\n    print(f\"\\n=== Training {model_name} on subset ===\")\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    train_dataset = df_to_dataset(train_df, tokenizer)\n    val_dataset = df_to_dataset(val_df, tokenizer)\n\n    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n\n    training_args = TrainingArguments(\n        output_dir=f\"./results/{model_name.replace('/', '_')}\",\n        eval_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        learning_rate=LR,\n        per_device_train_batch_size=BATCH_SIZE,\n        per_device_eval_batch_size=BATCH_SIZE,\n        num_train_epochs=EPOCHS,\n        weight_decay=0.01,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"f1\",\n        seed=SEED,\n        logging_dir=f\"./logs/{model_name.replace('/', '_')}\",\n        logging_steps=50,\n        report_to=\"none\"\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        tokenizer=tokenizer,\n        compute_metrics=compute_metrics\n    )\n\n    trainer.train()\n    metrics = trainer.evaluate()\n    results.append({\"model\": model_name, \"f1\": metrics[\"eval_f1\"], \"accuracy\": metrics[\"eval_accuracy\"]})\n\n# --------------------\n# SELECT BEST MODEL\n# --------------------\nresults_df = pd.DataFrame(results)\nprint(\"\\n=== Subset Results ===\")\nprint(results_df)\nbest_model_name = results_df.sort_values(by=\"f1\", ascending=False).iloc[0]['model']\nprint(f\"\\nBest model based on F1: {best_model_name}\")\n\n# --------------------\n# FULL DATA TRAINING\n# --------------------\nprint(\"\\n=== Training best model on full dataset ===\")\ntokenizer = AutoTokenizer.from_pretrained(best_model_name)\n\nfull_train_df, full_test_df = train_test_split(df, train_size=0.8, stratify=df['label'], random_state=SEED)\nfull_train_dataset = df_to_dataset(full_train_df, tokenizer)\nfull_test_dataset = df_to_dataset(full_test_df, tokenizer)\n\nmodel = AutoModelForSequenceClassification.from_pretrained(best_model_name, num_labels=2)\n\ntraining_args = TrainingArguments(\n    output_dir=f\"./final/{best_model_name.replace('/', '_')}\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=LR,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    num_train_epochs=EPOCHS,\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    seed=SEED,\n    logging_dir=f\"./logs/final_{best_model_name.replace('/', '_')}\",\n    logging_steps=50,\n    report_to=\"none\"\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=full_train_dataset,\n    eval_dataset=full_test_dataset,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()\nfinal_metrics = trainer.evaluate()\nprint(\"\\n=== Final Model Evaluation on Test Set ===\")\nprint(final_metrics)\n\n# --------------------\n# INFERENCE ON 10 RANDOM TEST REVIEWS\n# --------------------\nprint(\"\\n=== Inference on 10 random samples ===\")\nsample_df = full_test_df.sample(10, random_state=SEED)\nsample_dataset = Dataset.from_pandas(sample_df).map(\n    lambda x: tokenizer(x['review'], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH),\n    batched=True\n)\n\npreds = trainer.predict(sample_dataset)\npred_labels = np.argmax(preds.predictions, axis=-1)\npred_probs = torch.nn.functional.softmax(torch.tensor(preds.predictions), dim=-1).numpy()\n\nfor i, row in enumerate(sample_df.itertuples()):\n    print(f\"\\nReview: {row.review[:300]}...\")\n    print(f\"True Label: {row.label} | Predicted: {pred_labels[i]} | Confidence: {pred_probs[i][pred_labels[i]]:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T05:16:09.475264Z","iopub.execute_input":"2025-08-10T05:16:09.475549Z"}},"outputs":[{"name":"stdout","text":"Loading data...\n\n=== Training bert-base-uncased on subset ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0f21248c88b4d0e8019d64e5e743d44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"629b08636d4840ab8133656c2b76ed24"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_128/1387307531.py:100: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='525' max='525' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [525/525 08:03, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.280400</td>\n      <td>0.276317</td>\n      <td>0.895781</td>\n      <td>0.895833</td>\n      <td>0.896637</td>\n      <td>0.895833</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.163000</td>\n      <td>0.276997</td>\n      <td>0.904955</td>\n      <td>0.905000</td>\n      <td>0.905762</td>\n      <td>0.905000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.075500</td>\n      <td>0.315596</td>\n      <td>0.907497</td>\n      <td>0.907500</td>\n      <td>0.907555</td>\n      <td>0.907500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [38/38 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\n=== Training roberta-base on subset ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04e0e661859e4ea3a2f29eb4f88325ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4b378ff780d47a9aa467bf3679f8a9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40c1bfb6ea7146fcadb55e8718b2f6ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"355b832654524dcab697b8458f66f398"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef398265f6474a3ab8057e14f685216e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc8f6936cbd341cea012f23901e65c44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c659a0dec03c4fb193ea06679ee440c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5607739ab94a45ce85c4c884ab536e62"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_128/1387307531.py:100: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='525' max='525' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [525/525 08:33, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.224800</td>\n      <td>0.244903</td>\n      <td>0.922500</td>\n      <td>0.922500</td>\n      <td>0.922511</td>\n      <td>0.922500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.169500</td>\n      <td>0.244150</td>\n      <td>0.924155</td>\n      <td>0.924167</td>\n      <td>0.924432</td>\n      <td>0.924167</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.099700</td>\n      <td>0.259560</td>\n      <td>0.932489</td>\n      <td>0.932500</td>\n      <td>0.932770</td>\n      <td>0.932500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [38/38 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\n=== Training microsoft/deberta-v3-base on subset ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef183d69dfad411c9db4860186e6c211"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6be07f0d7c754012b81f356d3a671a6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"537dd7ec673940a6a5b1581692f9d991"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7786fba535784b519afa6c04175b1e19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a283d3d82d94e8285267faba70f3f10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cde700edebca4cab91db0fde8a2ef08a"}},"metadata":{}},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_128/1387307531.py:100: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1189181b97a4a8a868ea7c5751b0482"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='525' max='525' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [525/525 11:46, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.202000</td>\n      <td>0.177827</td>\n      <td>0.941656</td>\n      <td>0.941667</td>\n      <td>0.941981</td>\n      <td>0.941667</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.144500</td>\n      <td>0.162669</td>\n      <td>0.952498</td>\n      <td>0.952500</td>\n      <td>0.952562</td>\n      <td>0.952500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.061200</td>\n      <td>0.187746</td>\n      <td>0.949165</td>\n      <td>0.949167</td>\n      <td>0.949228</td>\n      <td>0.949167</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [38/38 00:15]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\n=== Training google/electra-base-discriminator on subset ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45b12389ff4d41fe9b05262e0eb36945"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8ecf42f5c84464f8ca4b524c9bfee51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee194e3912cc4b60867af24c753f88f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f603ffda3cc4d6f85e545422e30a25e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e30465c588b94a678a5b020a74e9cf01"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
